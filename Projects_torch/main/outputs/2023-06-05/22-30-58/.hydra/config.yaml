train_task:
  TrainTask:
    dataset_class:
      _target_: dataset.dataset.Dataset
      path2load:
      - /home/moshelaufer/PycharmProjects/datasets/noy_synth/
      type_files: wav
      labels: true
      dataset_names: tal_noise_1
    path2load_model: null
    model:
      _target_: Projects_torch.model.synth_tranformerE_model.SynthEncoder
      transformer:
        _target_: Projects_torch.layers.transformer.TransformerE
        d_model: 512
        num_heads: 8
        num_layers: 12
        d_ff: 2048
        max_seq_length: 130
        dropout: 0.1
      conv_encoder:
        _target_: Projects_torch.layers.conv_wav_encoder.ConvFeatureExtractionModel
        activation: relu
        units: 512
        conv_layers:
        - - 130
          - 512
          - 4
          - 2
        - - 512
          - 512
          - 3
          - 1
        - - 512
          - 512
          - 3
          - 1
        - - 512
          - 512
          - 2
          - 1
        num_duplicate_layer:
        - 1
        - 1
        - 1
        - 1
        dropout: 0.1
        conv_bias: true
        is_group_norm: false
        is_layer_norm: true
      linear_classifier:
        _target_: Projects_torch.layers.linear_classifier_2.LinearClassifier
        activation: relu
        dropout: 0.2
        outputs_dimension_per_outputs:
        - 3
        - 12
        - 20
        - 31
        - 4
        - 5
        - 8
        - 5
        - 16
    to_metrics: false
    metrics:
      _target_: Projects_torch.metrics.accuracy_2.CustomAccuracy2
      outputs_dimension_per_outputs:
      - 3
      - 12
      - 20
      - 31
      - 4
      - 5
      - 8
      - 5
      - 16
    loss:
    - _target_: Projects_torch.losses.l2_loss.L2Loss
    loss_ce:
      _target_: Projects_torch.losses.cross_entropy_loss_2.CELoss
      num_classes: 1
      index_y_true: 1
      outputs_dimension_per_outputs:
      - 3
      - 12
      - 20
      - 31
      - 4
      - 5
      - 8
      - 5
      - 16
    num_ce_loss: 1
    loss_l2: false
    callbacks:
    - _target_: tensorflow.keras.callbacks.ModelCheckpoint
      filepath: /home/moshelaufer/PycharmProjects/results/checkpoint/synth_autoencoder_noy/8/
      save_best_only: true
      save_freq: 7500
      initial_value_threshold: 5
      monitor: linear_classifier_loss
      verbose: 1
    to_schedule: true
    schedule:
      _target_: callbacks.WarmLRSchedule
      initial_learning_rate: 6.0e-06
      warmup_steps: 8
      hold_step: 1000
      decay_step: 10
      max_learn_rate: 7.5e-05
      min_learn_rate: 1.5e-06
    optimizer:
      _target_: tensorflow.keras.optimizers.Adam
      learning_rate: 0.0001
    processor:
      _target_: Projects_torch.processors.processor_synth_encoder_stft.DataLoaderSTFT
      masking: false
      norm_mean: 0.013329904
      norm_std: 0.041720923
      autoencoder: false
      mask:
      - 65
      - 1
      num_classes:
      - 3
      - 12
      - 20
      - 31
      - 4
      - 5
      - 8
      - 5
      - 16
    batch_size:
      train: 256
      test: 1
      valid: 64
    learning_rate: 0.001
    epochs: 4000
    path2save_model: /home/moshela/work/moshe/pycharm/checkpoints/noy/checkpoint_4.pt
    model_name: synth_autoencoder_2_stft_noy
    path2save_csv: /home/moshelaufer/PycharmProjects/results/csv/synth_autoencoder_2_stft/
    num_outputs: 2
    path2save_plot_model: /home/moshelaufer/PycharmProjects/results/plot/model_plot_tal_noise.png
    steps_per_epoch: 1000
    validation_steps: 150
    results:
      _target_: Projects_torch.results.results.Results
      path2save_results: /home/moshelaufer/PycharmProjects/results/results/results
      num_class: 100
optimizer:
  optimizer:
    _target_: optimizers.multi_optimizer.Optimizers
    indexes:
    - 1
    - 4
    - 5
    - 7
    ind_transformer_layer: 4
    optimizers:
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 9.88e-07
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 1.098e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 1.22e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 1.35e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 1.5e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 1.67e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 1.86e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 2.0667e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 2.296e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 2.55e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 2.83e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 3.15e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 3.5e-06
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 9.88e-07
          min_learn_rate: 3.0e-08
    - _target_: tensorflow.keras.optimizers.Adam
      learning_rate:
        _target_: tensorflow.keras.callbacks.LearningRateScheduler
        schedule:
          _target_: callbacks.WarmLRSchedule
          initial_learning_rate: 1.0e-07
          warmup_steps: 25
          hold_step: 1
          decay_step: 30
          max_learn_rate: 3.6e-06
          min_learn_rate: 3.0e-08
