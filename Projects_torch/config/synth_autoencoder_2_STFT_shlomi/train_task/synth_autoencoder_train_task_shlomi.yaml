# @package _group_

TrainTask:

  dataset_class:
    _target_: dataset.dataset.Dataset
    path2load:
      - /home/moshelaufer/PycharmProjects/datasets/tal_noise_2/
    type_files: wav
    labels: True
    dataset_names: tal_noise_1

  dataloader:
    _target_: Projects_torch.processors.processor_synth_encoder_stft.DataLoaderSTFT
    dataset_path: /home/moshelaufer/PycharmProjects/datasets/tal_noise_2/data/
    norm_mean: 0.026406644
    norm_std: 0.20221268
    num_classes:
      - 16
      - 4
      - 9
      - 4
      - 17
      - 17
      - 4
      - 17
      - 17

  devices: cuda:0

  saved_model: #/home/moshelaufer/PycharmProjects/results/checkpoint/synth_autoencoder_tal/

  model:
    _target_: Projects_torch.model.synth_autoencoder_model_2_stft.SynthAutoEncoder
#    inputs1:
#      - 130 #33
#      - 129 #40
#
#    inputs2:
#      - 65 #33
#      - 1

    conv_encoder:
      _target_: Projects_torch.layers.conv_wav_encoder.ConvFeatureExtractionModel
      activation: gelu
      units: 512
      conv_layers:
        -
          - 1
          - 512
          - 4
          - 2

        -
          - 512
          - 512
          - 3
          - 1

        -
          - 512
          - 512
          - 3
          - 1

        -
          - 512
          - 512
          - 2
          - 1

      num_duplicate_layer:
        - 1
        - 1
        - 1
        - 1

      dropout: 0.1
      conv_bias: True
      is_group_norm: True
      is_layer_norm: False

    conv_decoder:
      _target_: Projects_torch.layers.conv_wav_decoder.ConvDecoderModel
      activation: gelu
      units: 129
      conv_layers:
        -
          - 512
          - 512
          - 2
          - 1

        -
          - 512
          - 512
          - 3
          - 1

        -
          - 512
          - 512
          - 3
          - 1

        -
          - 512
          - 512
          - 4
          - 2

      num_duplicate_layer:
        - 1
        - 1
        - 1
        - 1

      dropout: 0.1
      conv_bias: True
      is_group_norm: True
      is_layer_norm: False

    transformer:
      _target_: Projects_torch.layers.transformer.Transformer
      num_transformer_blocks: 12
      d_model: 512
      num_heads: 16 #12
      d_ff: 3072
      dropout: 0.1
      activation: gelu

    linear_classifier:
      _target_: Projects_torch.layers.linear_classifier_2.LinearClassifier
      activation: relu
      dropout: 0.2
      outputs_dimension_per_outputs:
        - 16
        - 4
        - 9
        - 4
        - 17
        - 17
        - 4
        - 17
        - 17

  to_metrics: False #True #

  metrics:
    _target_: Projects_torch.metrics.accuracy_2.CustomAccuracy2
    outputs_dimension_per_outputs:
      - 16
      - 4
      - 9
      - 4
      - 17
      - 17
      - 4
      - 17
      - 17
#    num_classes: 1
#    index_y_true: 1

  loss:
#    - _target_: losses.spectral_magnitude_losses.Spectral_LogSTFTMagnitude_Loss
    - _target_: Projects_torch.losses.l2_loss.L2Loss

  loss_ce:
    _target_: Projects_torch.losses.cross_entropy_loss_2.CELoss
    num_classes: 1
    index_y_true: 1
    outputs_dimension_per_outputs:
      - 16
      - 4
      - 9
      - 4
      - 17
      - 17
      - 4
      - 17
      - 17

  num_ce_loss: 1

  callbacks:
    -
      _target_: tensorflow.keras.callbacks.ModelCheckpoint
      filepath: /home/moshelaufer/PycharmProjects/results/checkpoint/synth_autoencoder_shlomi/1/
      save_best_only: True
      save_freq: 4000 #'epoch'
      initial_value_threshold: 5
      monitor: 'linear_classifier_loss'
      verbose: 1
#    -
#      _target_: tensorflow.keras.callbacks.LearningRateScheduler
#      verbose: 1
#      schedule:
#        _target_: callbacks.scheduler
#        epoch: 0
#        lr: 1e-7

#    -
#      _target_: tensorflow.keras.callbacks.LearningRateScheduler
#      schedule:
#        _target_: callbacks.WarmLRSchedule
#        initial_learning_rate: 0.5e-6
#        warmup_steps: 25
#        hold_step: 1
#        decay_step: 200
#        max_learn_rate: 1.8e-6
#        min_learn_rate: 3e-8
#
#        _target_: tensorflow.keras.optimizers.schedules.ExponentialDecay
#        initial_learning_rate: 8.5e-5
#        decay_steps: 750
#        decay_rate: 1e-4

  to_schedule: True

  schedule:
    _target_: callbacks.WarmLRSchedule
    initial_learning_rate: 6e-6
    warmup_steps: 8
    hold_step: 1000 #1
    decay_step: 10
    max_learn_rate: 7.5e-5 #2.5e-5 #8e-6 #1.5e-6
    min_learn_rate: 1.5e-6 #5e-7


  optimizer:
    _target_: tensorflow.keras.optimizers.Adam
    learning_rate: 8e-6 #1.25e-6

  processor:
    _target_: processors.processor_synth_encoder_mask_2_stft.Processor #processors.processor_synth_encoder.Processor #
    norm_mean: 0.083749466
    norm_std: 0.4324925
    std_mean_calc:
      _target_: dataset.calc_std_mean_dataset_wav.StdMeanCalc
      path2dataset: /home/moshelaufer/PycharmProjects/datasets/tal_noise/
      stft: True
    mask:
      - 65 #33
      - 1
    num_classes:
      - 16
      - 4
      - 9
      - 4
      - 17
      - 17
      - 4
      - 17
      - 17

  batch_size:
    train: 58 #48
    test: 1
    valid: 58 #48

  epochs: 4000
  path2save_model: /home/moshelaufer/PycharmProjects/results/models/
  model_name: synth_autoencoder_2_stft_tal_shlomi
  path2save_csv: /home/moshelaufer/PycharmProjects/results/csv/synth_autoencoder_2_stft_shlomi/
  num_outputs: 2 #10 #1
  path2save_plot_model: /home/moshelaufer/PycharmProjects/results/plot/model_plot_shlomi.png

  steps_per_epoch: 1000 # 2500
  validation_steps: 150

  results:
    _target_: results.results.Results
    path2save_results: /home/moshelaufer/PycharmProjects/results/results/results
    num_class: 100

